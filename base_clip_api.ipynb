{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e46e8a09-efbf-403f-b89f-16d9ab2d8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "088810a8-ccf5-4420-be12-c0b26d1a6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff2a8183-d922-4d61-9247-97fd233749c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ba0fd1f-6030-4133-881a-2f68cd42dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Model\n",
    "device = \"cuda:0\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device, jit=False) \n",
    "#print('Official Preprocess Method: ', preprocess) ## official clip에서 사용한 preprocess\n",
    "#print('Official CLIP Model:', model)              ## clip (modifiedResNet + Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b008c60-c89c-4084-af02-1a40488595bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test load json\n",
    "import json\n",
    "json.loads('./config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89f19df-0161-47e1-9c55-ab72ed017021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sample Dataset\n",
    "df = pd.read_csv('./dataset/captions.txt', sep = '|')\n",
    "df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n",
    "df.to_csv('./dataset/captions.csv')\n",
    "image_path = './dataset/images'\n",
    "captions_path = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6ff4fb-39d1-41c2-ae64-dcf60104bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 Config 설정\n",
    "config = dict()\n",
    "config['dataset'] = '8k'\n",
    "config['save_path'] = 'results'\n",
    "\n",
    "model_config = {\n",
    "    'name': 'ModifiedResNet-Transformer'\n",
    "    'debug': False,\n",
    "    \n",
    "    'image_path': image_path,\n",
    "    'caption_path': captions_path,\n",
    "    \n",
    "    'batch_size': 64, # 조정\n",
    "    'num_workers': 4,\n",
    "    'head_lr': 1e-5,\n",
    "    'image_encoder_lr': 1e-4,\n",
    "    'text_encoder_lr': 1e-5,\n",
    "    'weight_decay': 1e-3,\n",
    "    \n",
    "    'patience': 1,\n",
    "    'factor': 0.8,\n",
    "    'epochs': 100,\n",
    "}\n",
    "\n",
    "config[\"model\"] = model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97fed77f-4958-4a9c-a6df-4eeddf511a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_caption_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "        self.image_path = list_image_path\n",
    "        self.caption  = clip.tokenize(list_txt) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_path[idx])) \n",
    "        caption = self.title[idx]\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eee19f7-b962-4ae8-8e3b-1b38a0c792b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = df[\"id\"].max() + 1 \n",
    "image_ids = np.arange(0, max_id)\n",
    "\n",
    "np.random.seed(42)\n",
    "valid_ids = np.random.choice(\n",
    "    image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47dee842-944d-4f60-b796-753fc9408554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train / test set\n",
    "train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "train_df = df[df[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "test_df = df[df[\"id\"].isin(valid_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ed9a470-6a6c-4575-bb95-22d6840fc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pytorch Dataset\n",
    "\n",
    "## Train Dataset\n",
    "train_image_tmp = train_df['image_name'].values.tolist()\n",
    "list_image_path = list()\n",
    "for file_nm in train_image_tmp :\n",
    "    file_nm_tmp = os.path.join('./dataset/images', file_nm)\n",
    "    list_image_path.append(file_nm_tmp)\n",
    "    \n",
    "list_txt = train_df['caption_text'].values.tolist()\n",
    "train_dataset = image_caption_dataset(list_image_path,list_txt)\n",
    "\n",
    "## Test Dataset\n",
    "test_image_tmp = test_df['image_name'].values.tolist()\n",
    "list_image_path = list()\n",
    "for file_nm in test_image_tmp :\n",
    "    file_nm_tmp = os.path.join('./dataset/images', file_nm)\n",
    "    list_image_path.append(file_nm_tmp)\n",
    "    \n",
    "list_txt = test_df['caption_text'].values.tolist()\n",
    "test_dataset = image_caption_dataset(list_image_path,list_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "964c0928-fe73-4cda-8061-e8372cffccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Train DataLoader\n",
    "mode = 'train'\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size = config['model']['batch_size'],\n",
    "                                              num_workers = config['model']['num_workers'],\n",
    "                                              shuffle=True if mode == 'train' else False)\n",
    "\n",
    "## Load Test DataLoader\n",
    "mode = 'test'\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size = config['model']['batch_size'],\n",
    "                                              num_workers = config['model']['num_workers'],\n",
    "                                              shuffle=True if mode == 'train' else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89e5edd3-51e1-424f-a0d5-c7b3059ea852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params, grads을 fp32로 바꿔주는 부분\n",
    "## attribute가 nan/inf로 바뀌는 에러 해결: https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_clip(model) :\n",
    "    for p in model.parameters() :\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "309714a9-d3b7-403f-ba54-0eee16dc3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp32로 바꾼 params, grads를 다시 원복\n",
    "clip.model.convert_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fb8ab3c-6c22-4750-967e-2fcbf6aa5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Loss\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=config['model']['patience'], factor=config['model']['factor'])\n",
    "step = 'epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5452348-3369-4afe-9c96-536846b1f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Func\n",
    "def train_epoch(config, model, train_loader, optimizer, lr_scheduler, step) :\n",
    "    \n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total = len(train_loader))\n",
    "    \n",
    "    for batch in tqdm_object :\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images, text = batch\n",
    "        \n",
    "        images = images.to(device)\n",
    "        text = text.to(device)\n",
    "        \n",
    "        logits_per_image, logits_per_text = model(images, text)\n",
    "        \n",
    "        target = torch.arange(len(images), dtype = torch.long, device = device)\n",
    "        \n",
    "        total_loss = (loss_img(logits_per_image, target) + loss_txt(logits_per_text, target))/2\n",
    "        total_loss.backward()\n",
    "        \n",
    "        convert_models_to_clip(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "        \n",
    "        if step == 'batch' :\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        count = images.size(0)\n",
    "        loss_meter.update(total_loss.item(), count)\n",
    "        \n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5785a461-78c0-4af8-addd-093eee5bab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test func\n",
    "def test_epoch(config, model, test_loader) :\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(test_loader, total=len(test_loader))\n",
    "    \n",
    "    for batch in tqdm_object :\n",
    "        \n",
    "        images, text = batch\n",
    "        \n",
    "        images = images.to(device)\n",
    "        text = text.to(device)\n",
    "\n",
    "        logits_per_image, logits_per_text  = model(images, text)\n",
    "        target = torch.arange(len(images), dtype = torch.long, device = device)\n",
    "        total_loss = (loss_img(logits_per_image, target) + loss_txt(logits_per_text, target))/2\n",
    "        \n",
    "        count = images.size(0)\n",
    "        loss_meter.update(total_loss.item(), count)\n",
    "        \n",
    "        tqdm_object.set_postfix(test_loss=loss_meter.avg)\n",
    "        \n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40707abe-f980-4564-8d14-a9fea9f3ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "wandb.init(project=\"clip-finetune-socar\", name=config['model']['name'])\n",
    "for epoch in range(config['model']['epochs']):\n",
    "    print(f\"# Epoch: {epoch + 1}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = train_epoch(config, model, train_loader, optimizer, lr_scheduler, step)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = test_epoch(config, model, test_loader)\n",
    "        \n",
    "    wandb.log({'loss/train': train_loss,\n",
    "              'loss/test': test_loss})\n",
    "    \n",
    "    ## best loss 기준으로 weight 저장\n",
    "    if test_loss.avg < best_loss :\n",
    "        best_loss = test_loss.avg\n",
    "        torch.save(model.state_dict(), f\"./{config['save_path']}/best_model.pth\")\n",
    "        print('Save best Model !')\n",
    "    \n",
    "    lr_scheduler.step(test_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d66fc-d966-49cc-b8c3-e76aaf6fec72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74964e-ce65-444c-b992-22c252ab6971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f103ee-cd41-42b3-9ee3-fcfd3e1ea6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
