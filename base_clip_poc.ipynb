{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7f14c6-7dd9-4ca9-bbef-3e74fd6ac640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm\n",
    "# !pip install transformers\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bd13bd-b618-4737-af81-b619e86d1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## For ImageEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "## For TextEncoder\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d452a7-79cc-4391-95e1-3982eec38717",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 구현한 모듈에서 가져오는 func\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from models.clip import *\n",
    "from models.ImageEncoder import *\n",
    "from models.TextEncoder import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b50900-5c03-4353-8e64-c3a5f7f7c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu111\n",
      "4.26.0\n",
      "0.6.12\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)        # 사용한 pytorch 버전체크 (1.8.1+cu111)\n",
    "print(transformers.__version__) # 사용한 transformer 버전체크(4.26.0)\n",
    "print(timm.__version__ )        # 사용한 timm 버전체크()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363bf6d5-f898-4fd6-b6f9-f93d7c8ad8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Requirements 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af6cc50-7b86-4112-bedf-3c89a6cac839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경우에 따라 달라질 수 있는 config 여기서 설정\n",
    "config = dict()\n",
    "config['dataset'] = '8k'\n",
    "config['save_path'] = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f3f921-0faa-40f8-adf6-e0cd08928419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092\n",
      "40455\n"
     ]
    }
   ],
   "source": [
    "# 불러온 데이터셋 기본적인 형태만 먼저 확인\n",
    "print(len(os.listdir('./dataset/images'))) # 이미지의 갯수\n",
    "df = pd.read_csv('./dataset/captions.txt', sep = '|')\n",
    "print(len(df)) # caption에 들어간 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d42cbf-3956-4619-8de4-f0d642c415db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset'] == '8k' :\n",
    "    df = pd.read_csv('./dataset/captions.txt', sep = '|')\n",
    "    df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n",
    "    df.to_csv('./dataset/captions.csv')\n",
    "    image_path = './dataset/images'\n",
    "    captions_path = './dataset'\n",
    "else:\n",
    "    raise ImplementationError(f'{config[\"dataset\"]} is not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11966cdf-d70a-45a6-8662-a25c202d91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config 추가 (image/text encoder + train 관련 configuration 모음)\n",
    "model_config = {\n",
    "\n",
    "    'debug': False,\n",
    "    \n",
    "    'image_path': image_path,\n",
    "    'caption_path': captions_path,\n",
    "    \n",
    "    'batch_size': 64, # 조정\n",
    "    'num_workers': 4,\n",
    "    'head_lr': 1e-5,\n",
    "    'image_encoder_lr': 1e-4,\n",
    "    'text_encoder_lr': 1e-5,\n",
    "    'weight_decay': 1e-3,\n",
    "    \n",
    "    'patience': 1,\n",
    "    'factor': 0.8,\n",
    "    'epochs': 100,\n",
    "    \n",
    "    'device': 'cuda:0',\n",
    "    \n",
    "    'model_name': 'resnet50',\n",
    "    'model_modify': False, # ResNet 변형버전 사용 여부\n",
    "    'image_embedding': 2048, # 모델에 따라 조정 (768, 2048)\n",
    "    'text_encoder_model': 'distilbert-base-uncased', \n",
    "    'text_embedding': 768, # 모델에 따라 조정\n",
    "    'text_tokenizer': 'distilbert-base-uncased',\n",
    "    'max_length': 200,\n",
    "    \n",
    "    'pretrained': True, # ImageEncoder, TextEncoder 모두적용\n",
    "    'trainable': True, \n",
    "    'temperature': 0.5,\n",
    "    \n",
    "    'image_size': 224,\n",
    "    \n",
    "    'num_projection_layers': 1,\n",
    "    'projection_dim': 256,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "config[\"model\"] = model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb76a68e-1315-427c-870f-674c79dab876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset (Input으로 들어갈 수 있는 형태로)\n",
    "max_id = df[\"id\"].max() + 1 if not config[\"model\"][\"debug\"] else 100\n",
    "image_ids = np.arange(0, max_id)\n",
    "\n",
    "np.random.seed(42)\n",
    "valid_ids = np.random.choice(\n",
    "    image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86db6be-d4d2-4f28-8913-f76f358435af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "train_df = df[df[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "test_df = df[df[\"id\"].isin(valid_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "449ded85-8c5e-4237-bc3b-ec8e1e065847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CLIP func\n",
    "def train_epoch(config, model, train_loader, optimizer, lr_scheduler, step) :\n",
    "    \n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total = len(train_loader))\n",
    "    \n",
    "    for batch in tqdm_object :\n",
    "        batch = {k: v.to(config[\"model\"][\"device\"]) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step == 'batch' :\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "        \n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02ba4f38-9a52-4829-94a8-16546323bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CLIP func\n",
    "def test_epoch(config, model, test_loader) :\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(test_loader, total=len(test_loader))\n",
    "    \n",
    "    for batch in tqdm_object :\n",
    "        batch = {k: v.to(config[\"model\"][\"device\"]) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        \n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "        \n",
    "        tqdm_object.set_postfix(test_loss=loss_meter.avg)\n",
    "        \n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d75b4bf-36d9-4d53-b105-a95189a7bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main에 들어갈 부분 (실행)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(config[\"model\"][\"text_tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de0ae9fc-f95f-4cb6-805a-034d0d700247",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns={'image_name':'image'})\n",
    "train_df = train_df.rename(columns={'caption_text':'caption'})\n",
    "\n",
    "test_df = test_df.rename(columns={'image_name':'image'})\n",
    "test_df = test_df.rename(columns={'caption_text':'caption'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0887c72-2a86-43d7-b58f-f291e4495776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = build_loaders(config, train_df, tokenizer, mode=\"train\")\n",
    "test_loader = build_loaders(config, test_df, tokenizer, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3306a767-b265-42bc-98c4-d11dfb48792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Check Image Encoder: resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Check Text Encoder: distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "model = CLIP(config).to(config[\"model\"][\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bb742c8-3a81-48f6-bf32-32ad843a53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 옵션들 \n",
    "params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": config['model']['image_encoder_lr']},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": config['model']['text_encoder_lr']},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.image_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": config['model']['head_lr'], \"weight_decay\": config['model']['head_lr']}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caf07bc9-d109-4851-8292-22ba612e1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=config['model']['patience'], factor=config['model']['factor'])\n",
    "step = \"epoch\" # 로깅 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc249d-0ed1-47aa-9cf0-2262c086cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(config['model']['epochs']):\n",
    "    print(f\"# Epoch: {epoch + 1}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = train_epoch(config, model, train_loader, optimizer, lr_scheduler, step)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = test_epoch(config, model, test_loader)\n",
    "    \n",
    "    ## best loss 기준으로 weight 저장\n",
    "    if test_loss.avg < best_loss :\n",
    "        best_loss = test_loss.avg\n",
    "        torch.save(model.state_dict(), f\"./{config['save_path']}/best_model.pth\")\n",
    "        print('Save best Model !')\n",
    "    \n",
    "    lr_scheduler.step(test_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1a7e5-83a3-4a6e-bf24-f02b5e299af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f250661-3e4b-46dd-937a-b92355b409ef",
   "metadata": {},
   "source": [
    "### Inference (CIFAR10 Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3bbd72-db02-4fb1-8594-84bc7d413668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from pkg_resources import packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c791303-650c-46ec-b406-672714a0e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Check Image Encoder: resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Check Text Encoder: distilbert-base-uncased\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step0. Load model\n",
    "model = CLIP(config).to(config[\"model\"][\"device\"])\n",
    "model.load_state_dict(torch.load('./results/best_model_resnet50_bert.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eca6bc2-bb54-4fd9-bfac-fa46a8748e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. Find Class and Templates\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "cifar10_templates = ['a photo of {}.'] ## 추가가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be10b8c4-1a8c-4626-bc10-a71c4ca66d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. Load Dataset\n",
    "test_root = './dataset/cifar10/Test'\n",
    "\n",
    "test_transform_option = transforms.Compose([\n",
    "                        transforms.Resize((32, 32)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761])\n",
    "                    ])\n",
    "test_datasets = torchvision.datasets.ImageFolder(root=test_root, transform = test_transform_option)\n",
    "test_loader = torch.utils.data.DataLoader(test_datasets, batch_size = 256, shuffle=False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24f6e385-9285-4a2e-8c5d-8e3c3cb9dd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc5527497874994bd9f9719a3824e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step3. Create zero shot classifier weight\n",
    "def zeroshot_classifier(classnames, templates, model):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname) for template in templates] \n",
    "            \n",
    "            tokenizer = DistilBertTokenizer.from_pretrained(config['model']['text_tokenizer'])\n",
    "            encoded_query = tokenizer(texts)\n",
    "            \n",
    "            batch = {\n",
    "                key: torch.tensor(values).to(config['model']['device'])\n",
    "                for key, values in encoded_query.items()\n",
    "            }\n",
    "            \n",
    "            text_features = model.text_encoder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            class_embeddings = model.text_projection(text_features)\n",
    "            \n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            \n",
    "            zeroshot_weights.append(class_embedding)\n",
    "            \n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights\n",
    "\n",
    "\n",
    "zeroshot_weights = zeroshot_classifier(cifar10_classes, cifar10_templates, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d775b557-7bd2-475f-9325-0de107286e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80bcc304-ebc7-4735-a793-640a47787feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3c00f1f65f455aa63059b927afcb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 10])\n",
      "torch.Size([256, 2048])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_embeddings_n\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_embeddings_n\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m dot_sim \u001b[38;5;241m=\u001b[39m text_embeddings_n \u001b[38;5;241m@\u001b[39m image_embeddings_n\n\u001b[1;32m     16\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m dot_sim\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# batch size\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "# Step4. Zero shot prediction\n",
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(test_loader)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.image_encoder(images)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        image_embeddings_n = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_embeddings_n = F.normalize(zeroshot_weights, p=2, dim=-1)\n",
    "        print(text_embeddings_n.shape)\n",
    "        print(image_embeddings_n.shape)\n",
    "        dot_sim = text_embeddings_n @ image_embeddings_n\n",
    "        logits = 100. * dot_sim\n",
    "        print(logits.shape) # batch size\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1,))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2270cf-6ac0-4075-b4b0-3f150935523f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29a138-40f7-4e42-8284-89e7fd215ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e1c38-e420-4de9-8234-8d1315d3ce52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
